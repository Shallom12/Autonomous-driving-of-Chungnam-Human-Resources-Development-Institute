# -*- coding: utf-8 -*-
"""인공지능과 경영 최종 프로젝트ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sS4GItoCVsGK5hwF2_REMwAkLmwyLoKh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
upload=files.upload()
dataset= pd.read_csv("TravelInsurancePrediction.csv")
dataset.head()

dataset.drop(columns=["Unnamed: 0"], inplace=True)
dataset.head()

dataset.info()

dataset["Employment Type"]=dataset["Employment Type"].map({"Government Sector":0, "Private Sector/Self Employed":1})
dataset["GraduateOrNot"]=dataset["GraduateOrNot"].map({"No":0, "Yes":1})
dataset["FrequentFlyer"]=dataset["FrequentFlyer"].map({"No":0, "Yes":1})
dataset["EverTravelledAbroad"]=dataset["EverTravelledAbroad"].map({"No":0, "Yes":1})

dataset.head()

## divide the dataset into independent and dependent features
X = dataset.iloc[:,:8]
y = dataset.iloc[:,8]
print(X, y)

# 필요한 라이브러리 임포트
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten,Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from google.colab import files
upload=files.upload()
# 데이터 로드 및 전처리
dataset = pd.read_csv("TravelInsurancePrediction.csv")

# 범주형 데이터 맵핑
dataset["Employment Type"] = dataset["Employment Type"].map({"Government Sector": 0, "Private Sector/Self Employed": 1})
dataset["GraduateOrNot"] = dataset["GraduateOrNot"].map({"No": 0, "Yes": 1})
dataset["FrequentFlyer"] = dataset["FrequentFlyer"].map({"No": 0, "Yes": 1})
dataset["EverTravelledAbroad"] = dataset["EverTravelledAbroad"].map({"No": 0, "Yes": 1})

# 독립 변수(X)와 종속 변수(y) 분리
X = dataset.iloc[:, :8]
y = dataset.iloc[:, 8]
# 데이터 스케일링
scaler = StandardScaler()
X = scaler.fit_transform(X)
# 종속 변수 원-핫 인코딩
y = to_categorical(y)
# 학습 및 테스트 세트 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 딥러닝 모델 구축
def build_model():
    model = Sequential()
    model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.4))
    model.add(Dense(y_train.shape[1], activation='softmax'))
    model.summary()
    model.compile(optimizer='adam', loss='categorical_crossentropy'
    , metrics=['accuracy'])
    return model
model = build_model()

# 모델 학습
history = model.fit(X_train, y_train, epochs=300, batch_size=155,
                    validation_split=0.2, verbose=1)

# 테스트 세트 평가
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy:.2f}")

# 예측 및 평가 보고서 생성
y_pred = np.argmax(model.predict(X_test), axis=-1)
y_true = np.argmax(y_test, axis=-1)
print(classification_report(y_true, y_pred))

# 학습 과정 시각화
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()
plt.show()