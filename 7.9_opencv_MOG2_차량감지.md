%pip install opencv-python

# 1. 기본 MOG2 차량 감지 코드
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt


# 동영상 파일 열기 (코랩에서는 업로드한 파일 경로 사용)
cap = cv2.VideoCapture('/content/sample_data/around5.mp4')  # 파일 경로 수정 필요


# MOG2 배경 차분기 생성
backSub = cv2.createBackgroundSubtractorMOG2()


frame_count = 0
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # 배경 차분 적용
    fgMask = backSub.apply(frame)
        # 노이즈 제거 (모폴로지 연산)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)  # 작은 노이즈 제거
    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)  # 구멍 메우기

      # 윤곽선 검출 및 바운딩 박스
    contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # 바운딩 박스를 그릴 프레임 복사
    result_frame = frame.copy()

    for contour in contours:
        # 너무 작은 영역은 제외 (차량이 아닐 가능성 높음)
        if cv2.contourArea(contour) > 3000:
            x, y, w, h = cv2.boundingRect(contour)
            cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

    # 매 30프레임마다 결과 출력 (너무 많은 출력 방지)
    if frame_count % 30 == 0:   # 30초마다 1프레임
        # 결과를 나란히 표시
        combined = np.hstack((frame, cv2.cvtColor(fgMask, cv2.COLOR_GRAY2BGR)))
        cv2_imshow(combined)

    frame_count += 1

    # 100프레임 정도만 처리 (테스트용)
    if frame_count > 100:
        break


# 자원 해제
cap.release()
print("처리 완료!")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt


# 동영상 파일 열기 (코랩에서는 업로드한 파일 경로 사용)
# 실제 코랩 환경에 업로드된 파일 경로로 수정해주세요.
video_path = '/content/sample_data/around5.mp4'
cap = cv2.VideoCapture(video_path)

# 동영상 파일이 제대로 열렸는지 확인
if not cap.isOpened():
    print(f"오류: 동영상 파일을 열 수 없습니다. 경로를 확인해주세요: {video_path}")
    exit()

# MOG2 배경 차분기 생성
# history: 배경 모델 학습에 사용할 프레임 수 (과거 500 프레임)
# varThreshold: 픽셀과 배경 모델 간의 마할라노비스 거리 임계값 (클수록 덜 민감)
# detectShadows: 그림자 감지 여부 (차량만 감지하려면 False가 더 좋음)
backSub = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=False)

frame_count = 0
display_interval = 30 # 결과를 표시할 프레임 간격 (30프레임마다 1번)
max_frames_to_process = 300 # 테스트를 위해 처리할 최대 프레임 수

print("차량 감지 처리 시작...")

while True:
    ret, frame = cap.read()
    if not ret:
        print("프레임을 더 이상 읽을 수 없거나 동영상이 끝났습니다.")
        break

    # 배경 차분 적용
    # 학습률(learningRate)을 -1로 설정하여 자동 학습
    fgMask = backSub.apply(frame, learningRate=-1)

    # 노이즈 제거 및 형태학적 연산으로 객체 강조
    # 모폴로지 커널 정의
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)) # 커널 크기 증가 (3,3 -> 5,5)

    # 이진화: 배경 차분 마스크의 노이즈를 줄이고 객체를 명확히 하기 위해 이진 임계값 적용
    # 픽셀 값이 127보다 크면 255 (흰색), 아니면 0 (검은색)
    _, fgMask = cv2.threshold(fgMask, 127, 255, cv2.THRESH_BINARY)


    # 모폴로지 연산 순서 변경 및 추가:
    # 1. Opening: 작은 노이즈 제거 (침식 후 팽창)
    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel, iterations=2) # 반복 횟수 증가
    # 2. Closing: 객체 내부의 작은 구멍 메우기 (팽창 후 침식)
    fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel, iterations=3) # 반복 횟수 증가


    # 윤곽선 검출 및 바운딩 박스
    # cv2.RETR_EXTERNAL: 가장 바깥쪽 윤곽선만 검출 (내부 윤곽선 무시)
    # cv2.CHAIN_APPROX_SIMPLE: 수평, 수직, 대각선 세그먼트의 끝점만 저장하여 메모리 절약
    contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # 바운딩 박스를 그릴 프레임 복사
    result_frame = frame.copy()

    # 감지된 차량 개수 카운트
    detected_cars = 0
    # 유효한 윤곽선 영역의 최소값 설정 (픽셀 단위)
    min_contour_area = 1500 # 이 값을 조정하여 작은 노이즈 또는 너무 작은 객체를 필터링

    for contour in contours:
        # 윤곽선 영역 계산
        area = cv2.contourArea(contour)

        # 설정된 최소 영역보다 큰 윤곽선만 처리 (차량이 아닐 가능성 높은 작은 영역 제외)
        if area > min_contour_area:
            x, y, w, h = cv2.boundingRect(contour)

            # 바운딩 박스 종횡비(가로/세로 비율) 확인 (선택 사항: 차량 모양에 따라 필터링)
            # typical_aspect_ratio_min = 0.5 # 예시 값
            # typical_aspect_ratio_max = 3.0 # 예시 값
            # aspect_ratio = w / h
            # if typical_aspect_ratio_min < aspect_ratio < typical_aspect_ratio_max:

            # 바운딩 박스 그리기: (0, 255, 0)은 초록색, 2는 선 두께
            cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            detected_cars += 1

    # 프레임에 감지된 차량 수 표시
    cv2.putText(result_frame, f"Cars Detected: {detected_cars}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)


    # 매 display_interval 프레임마다 결과 출력
    if frame_count % display_interval == 0:
        # 결과를 나란히 표시: 원본 프레임과 배경 차분 마스크
        # fgMask는 흑백이므로 컬러로 변환하여 합쳐야 함
        combined_display = np.hstack((frame, cv2.cvtColor(fgMask, cv2.COLOR_GRAY2BGR), result_frame))
        cv2_imshow(combined_display)

    frame_count += 1

    # 설정된 최대 프레임 수만큼만 처리
    if frame_count >= max_frames_to_process:
        print(f"{max_frames_to_process} 프레임 처리 완료. 종료합니다.")
        break


# 자원 해제
cap.release()
cv2.destroyAllWindows() # 모든 OpenCV 창 닫기 (코랩에서는 큰 영향 없음)
print("처리 완료!")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow # Google Colab 환경을 위한 라이브러리

# --- Helper Functions (이전 코드와 동일) ---

def region_of_interest(img, vertices):
    """
    이미지에서 관심 영역(ROI)을 추출합니다.
    """
    mask = np.zeros_like(img)
    match_mask_color = 255
    cv2.fillPoly(mask, vertices, match_mask_color)
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

def draw_lines(img, lines, color=(0, 255, 0), thickness=5):
    """
    감지된 차선을 이미지에 그립니다.
    """
    if lines is None:
        return img
    line_img = np.zeros_like(img)
    for line in lines:
        for x1, y1, x2, y2 in line:
            cv2.line(line_img, (x1, y1), (x2, y2), color, thickness)
    # 원본 이미지와 차선 이미지를 합성합니다. (투명도 적용)
    img = cv2.addWeighted(img, 0.8, line_img, 1.0, 0.0)
    return img

def draw_lane_bounding_box(img, lines, color=(255, 0, 0), thickness=2):
    """
    감지된 모든 차선을 포함하는 하나의 네모 박스를 그립니다.
    """
    if lines is None or len(lines) == 0:
        return img

    all_x = []
    all_y = []
    for line in lines:
        for x1, y1, x2, y2 in line:
            all_x.extend([x1, x2])
            all_y.extend([y1, y2])

    if not all_x or not all_y:
        return img

    min_x = min(all_x)
    max_x = max(all_x)
    min_y = min(all_y)
    max_y = max(all_y)

    # 차선 영역에 약간의 여유를 주기 위해 확장
    padding = 10
    min_x = max(0, min_x - padding)
    max_x = min(img.shape[1], max_x + padding)
    min_y = max(0, min_y - padding)
    max_y = min(img.shape[0], max_y + padding)

    cv2.rectangle(img, (min_x, min_y), (max_x, max_y), color, thickness)
    return img

def draw_roi_polygon_on_image(img, vertices, color=(255, 0, 0), thickness=2):
    """
    원본 이미지에 ROI 다각형(파란색)을 그립니다.
    """
    img_with_roi = img.copy()
    cv2.polylines(img_with_roi, [np.array(vertices, np.int32)], True, color, thickness)
    return img_with_roi

def add_text_overlay(img, text, position, font_scale=0.7, color=(255, 255, 255), thickness=2):
    """
    이미지에 텍스트 오버레이를 추가합니다.
    """
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(img, text, position, font, font_scale, color, thickness, cv2.LINE_AA)
    return img

# --- Main Processing Loop ---

video_path = '/content/sample_data/around5.mp4'
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print(f"오류: 동영상 파일을 열 수 없습니다. 경로를 확인해주세요: {video_path}")
    exit()

frame_count = 0
max_frames_to_process = 300
display_interval = 30

print("차선 인식 처리 시작...")

while True:
    ret, frame = cap.read()
    if not ret:
        print("프레임을 더 이상 읽을 수 없거나 동영상이 끝났습니다.")
        break

    height, width = frame.shape[:2]

    # --- 1. 이미지 전처리 ---
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    blur_gray = cv2.GaussianBlur(gray_frame, (5, 5), 0)

    # --- 2. 엣지 검출 (Canny) ---
    edges = cv2.Canny(blur_gray, 50, 150)

    # --- 3. 관심 영역(ROI) 설정 - 사용자 요청에 따라 재조정 ---
    # "앞쪽으로 조금 더 길게 인식" -> 상단 Y좌표를 위로 이동 (0.65 -> 0.60)
    # "왼쪽너비를 더 줄이고" -> 좌측 하단 x좌표를 오른쪽으로 이동 (0.15 -> 0.20)
    # -> 좌측 상단 x좌표를 오른쪽으로 이동 (0.35 -> 0.40)
    # "오른쪽으로 더 너비를 늘려서 인식" -> 우측 상단 x좌표를 오른쪽으로 이동 (0.65 -> 0.70)
    # -> 우측 하단 x좌표를 오른쪽으로 확장 (0.95 -> 0.98)
    roi_vertices = [
        (int(width * 0.20), height),             # 좌측 하단 (더 오른쪽으로 이동, 왼쪽 너비 더 감소)
        (int(width * 0.40), int(height * 0.64)), # 좌측 상단 (더 오른쪽으로 이동, 앞쪽을 더 길게 인식)
        (int(width * 0.70), int(height * 0.64)), # 우측 상단 (더 오른쪽으로 이동, 앞쪽을 더 길게 인식)
        (int(width * 0.98), height)              # 우측 하단 (더 오른쪽으로 확장, 오른쪽 너비 더 증가)
    ]
    masked_edges = region_of_interest(edges, np.array([roi_vertices], np.int32))

    # --- 4. Hough 변환을 이용한 직선 검출 ---
    lines = cv2.HoughLinesP(masked_edges, 1, np.pi/180, 50, minLineLength=80, maxLineGap=10)

    # --- 5. 각 단계별 이미지 준비 (시각화 목적) ---

    # 첫 번째 이미지: 원본 프레임에 조정된 ROI 다각형 표시 및 텍스트 추가
    display_img1 = draw_roi_polygon_on_image(frame.copy(), roi_vertices, color=(0, 0, 255), thickness=2) # 파란색 ROI
    display_img1 = add_text_overlay(display_img1, "Original ROI (Adjusted)", (10, height - 20), color=(255, 255, 255))
    display_img1 = add_text_overlay(display_img1, "Canny Edge Detection", (width - 250, 30), color=(255, 255, 255))

    # 두 번째 이미지: ROI가 적용된 Canny 엣지 결과 (흑백을 컬러로 변환)
    display_img2 = cv2.cvtColor(masked_edges, cv2.COLOR_GRAY2BGR)
    display_img2 = add_text_overlay(display_img2, "Canny Edges (ROI Applied)", (10, height - 20), color=(255, 255, 255))
    display_img2 = add_text_overlay(display_img2, "Processing Step", (width - 250, 30), color=(255, 255, 255))


    # 세 번째 이미지: 최종 차선 감지 결과 (차선 선 + 네모 박스)
    display_img3 = draw_lines(frame.copy(), lines) # 원본 프레임에 초록색 차선 선 그리기
    display_img3 = draw_lane_bounding_box(display_img3, lines, color=(255, 0, 0), thickness=2) # 감지된 모든 차선 영역에 빨간색 네모 박스 그리기
    display_img3 = add_text_overlay(display_img3, "Lane Detection Result", (10, height - 20), color=(255, 255, 255))
    display_img3 = add_text_overlay(display_img3, "Final Output", (width - 250, 30), color=(255, 255, 255))


    # --- 6. 세 이미지를 수직으로 쌓아서 표시 ---
    if frame_count % display_interval == 0:
        combined_vertical_display = np.vstack((display_img1, display_img2, display_img3))
        cv2_imshow(combined_vertical_display)

    frame_count += 1

    if frame_count >= max_frames_to_process:
        print(f"{max_frames_to_process} 프레임 처리 완료. 종료합니다.")
        break

# 자원 해제
cap.release()
# cv2.destroyAllWindows() # This function is not needed in Colab and causes an error.
print("차선 인식 처리 완료!")

import cv2
import numpy as np
from google.colab.patches import cv2_imshow # Google Colab 환경을 위한 라이브러리

# --- Helper Functions (이전 코드와 동일) ---

def region_of_interest(img, vertices):
    """
    이미지에서 관심 영역(ROI)을 추출합니다.
    """
    mask = np.zeros_like(img)
    match_mask_color = 255
    cv2.fillPoly(mask, vertices, match_mask_color)
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

def draw_lines(img, lines, color=(0, 255, 0), thickness=5):
    """
    감지된 차선을 이미지에 그립니다.
    """
    if lines is None:
        return img
    line_img = np.zeros_like(img)
    for line in lines:
        for x1, y1, x2, y2 in line:
            cv2.line(line_img, (x1, y1), (x2, y2), color, thickness)
    # 원본 이미지와 차선 이미지를 합성합니다. (투명도 적용)
    img = cv2.addWeighted(img, 0.8, line_img, 1.0, 0.0)
    return img

def draw_lane_bounding_box(img, lines, color=(255, 0, 0), thickness=2):
    """
    감지된 모든 차선을 포함하는 하나의 네모 박스를 그립니다.
    """
    if lines is None or len(lines) == 0:
        return img

    all_x = []
    all_y = []
    for line in lines:
        for x1, y1, x2, y2 in line:
            all_x.extend([x1, x2])
            all_y.extend([y1, y2])

    if not all_x or not all_y:
        return img

    min_x = min(all_x)
    max_x = max(all_x)
    min_y = min(all_y)
    max_y = max(all_y)

    # 차선 영역에 약간의 여유를 주기 위해 확장
    padding = 10
    min_x = max(0, min_x - padding)
    max_x = min(img.shape[1], max_x + padding)
    min_y = max(0, min_y - padding)
    max_y = min(img.shape[0], max_y + padding)

    cv2.rectangle(img, (min_x, min_y), (max_x, max_y), color, thickness)
    return img

def draw_roi_polygon_on_image(img, vertices, color=(255, 0, 0), thickness=2):
    """
    원본 이미지에 ROI 다각형(파란색)을 그립니다.
    """
    img_with_roi = img.copy()
    # np.array를 한 번 더 감싸서 리스트 형태로 전달
    cv2.polylines(img_with_roi, [np.array(vertices, np.int32)], True, color, thickness)
    return img_with_roi

def add_text_overlay(img, text, position, font_scale=0.7, color=(255, 255, 255), thickness=2):
    """
    이미지에 텍스트 오버레이를 추가합니다.
    """
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(img, text, position, font, font_scale, color, thickness, cv2.LINE_AA)
    return img

# --- Main Processing Loop ---

video_path = '/content/sample_data/around5.mp4'# 파일 경로 확인
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print(f"오류: 동영상 파일을 열 수 없습니다. 경로를 확인해주세요: {video_path}")
    exit()

frame_count = 0
max_frames_to_process = 300
display_interval = 30

print("차선 인식 처리 시작...")

while True:
    ret, frame = cap.read()
    if not ret:
        print("프레임을 더 이상 읽을 수 없거나 동영상이 끝났습니다.")
        break

    height, width = frame.shape[:2]

    # --- 1. 이미지 전처리 ---
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    blur_gray = cv2.GaussianBlur(gray_frame, (5, 5), 0)

    # --- 2. 엣지 검출 (Canny) ---
    edges = cv2.Canny(blur_gray, 50, 150)

    # --- 3. 관심 영역(ROI) 설정 - 삼각형 모양으로 변경 ---
    # 이전 사다리꼴의 상단 두 점을 하나의 정점(apex)으로 합칩니다.
    # 이전 상단 좌우 x좌표: 0.40, 0.70
    # 이전 상단 y좌표: 0.60
    
    # 삼각형의 꼭짓점 (상단 중앙)
    apex_x = int(width * ((0.40 + 0.70) / 2)) # 이전 상단 좌우 x좌표의 중간
    apex_y = int(height * 0.60)              # 이전 상단 y좌표 유지

    # 삼각형의 세 꼭짓점 정의: 좌측 하단, 상단 중앙(꼭짓점), 우측 하단
    roi_vertices = [
        (int(width * 0.20), height),             # 좌측 하단
        (apex_x, apex_y),                        # 상단 중앙 (꼭짓점)
        (int(width * 0.98), height)              # 우측 하단
    ]
    masked_edges = region_of_interest(edges, np.array([roi_vertices], np.int32))

    # --- 4. Hough 변환을 이용한 직선 검출 ---
    lines = cv2.HoughLinesP(masked_edges, 1, np.pi/180, 50, minLineLength=80, maxLineGap=10)

    # --- 5. 각 단계별 이미지 준비 (시각화 목적) ---

    # 첫 번째 이미지: 원본 프레임에 조정된 ROI 다각형 표시 및 텍스트 추가
    display_img1 = draw_roi_polygon_on_image(frame.copy(), roi_vertices, color=(0, 0, 255), thickness=2) # 파란색 ROI
    display_img1 = add_text_overlay(display_img1, "Original ROI (Triangular)", (10, height - 20), color=(255, 255, 255))
    display_img1 = add_text_overlay(display_img1, "Canny Edge Detection", (width - 250, 30), color=(255, 255, 255))

    # 두 번째 이미지: ROI가 적용된 Canny 엣지 결과 (흑백을 컬러로 변환)
    display_img2 = cv2.cvtColor(masked_edges, cv2.COLOR_GRAY2BGR)
    display_img2 = add_text_overlay(display_img2, "Canny Edges (ROI Applied)", (10, height - 20), color=(255, 255, 255))
    display_img2 = add_text_overlay(display_img2, "Processing Step", (width - 250, 30), color=(255, 255, 255))


    # 세 번째 이미지: 최종 차선 감지 결과 (차선 선 + 네모 박스)
    display_img3 = draw_lines(frame.copy(), lines) # 원본 프레임에 초록색 차선 선 그리기
    display_img3 = draw_lane_bounding_box(display_img3, lines, color=(255, 0, 0), thickness=2) # 감지된 모든 차선 영역에 빨간색 네모 박스 그리기
    display_img3 = add_text_overlay(display_img3, "Lane Detection Result", (10, height - 20), color=(255, 255, 255))
    display_img3 = add_text_overlay(display_img3, "Final Output", (width - 250, 30), color=(255, 255, 255))


    # --- 6. 세 이미지를 수직으로 쌓아서 표시 ---
    if frame_count % display_interval == 0:
        combined_vertical_display = np.vstack((display_img1, display_img2, display_img3))
        cv2_imshow(combined_vertical_display)

    frame_count += 1

    if frame_count >= max_frames_to_process:
        print(f"{max_frames_to_process} 프레임 처리 완료. 종료합니다.")
        break

# 자원 해제
cap.release()
cv2.destroyAllWindows()
print("차선 인식 처리 완료!")
