# ë¨¸ì‹ ëŸ¬ë‹ ì™„ì „ ì •ë³µ ê°€ì´ë“œ ğŸ§ 
*ë¹„ì§€ë„í•™ìŠµê³¼ ê°•í™”í•™ìŠµì„ í†µí•œ ì¸ê³µì§€ëŠ¥ì˜ ê¹Šì€ ì´í•´*

```ascii
    ğŸ¯ MACHINE LEARNING PARADIGMS ğŸ¯
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ·ï¸ Supervised Learning â†’ ğŸ“Š Labeled Data Training  â”‚
    â”‚  ğŸ” Unsupervised Learning â†’ ğŸ§© Pattern Discovery    â”‚
    â”‚  ğŸ® Reinforcement Learning â†’ ğŸ† Reward Optimization â”‚
    â”‚                                                     â”‚
    â”‚  Human Teacher â†’ Self Discovery â†’ Trial & Error     â”‚
    â”‚       â†“              â†“                â†“            â”‚
    â”‚   Classification â†’ Clustering â†’ Decision Making     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ í•™ìŠµ ëª©í‘œì™€ ë¡œë“œë§µ

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ì—¬ëŸ¬ë¶„ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë‘ ê°€ì§€ í•µì‹¬ íŒ¨ëŸ¬ë‹¤ì„ì¸ ë¹„ì§€ë„í•™ìŠµê³¼ ê°•í™”í•™ìŠµì„ ë§ˆìŠ¤í„°í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. ë§ˆì¹˜ ì•„ì´ê°€ ì–¸ì–´ë¥¼ ë°°ìš°ëŠ” ê³¼ì •ì²˜ëŸ¼, ìš°ë¦¬ëŠ” ë‹¨ê³„ë³„ë¡œ ì´í•´ì˜ ê¹Šì´ë¥¼ ë”í•´ë‚˜ê°€ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ì „ì²´ì ì¸ í•™ìŠµ ì—¬ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

**1ë‹¨ê³„: ê¸°ì´ˆ ì´í•´** - ê° í•™ìŠµ ë°©ë²•ì˜ ë³¸ì§ˆì  íŠ¹ì„±ê³¼ ì² í•™ì„ ì´í•´í•©ë‹ˆë‹¤
**2ë‹¨ê³„: êµ¬ì²´ì  ì•Œê³ ë¦¬ì¦˜** - ì‹¤ì œ ì‘ë™ ì›ë¦¬ì™€ ìˆ˜í•™ì  ë°°ê²½ì„ í•™ìŠµí•©ë‹ˆë‹¤  
**3ë‹¨ê³„: ì‹¤ì „ ì‘ìš©** - í˜„ì‹¤ ë¬¸ì œì— ì ìš©í•˜ëŠ” ë°©ë²•ì„ ìµí™ë‹ˆë‹¤
**4ë‹¨ê³„: ê³ ê¸‰ ìµœì í™”** - ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ë¬¸ê°€ ê¸°ë²•ì„ ìŠµë“í•©ë‹ˆë‹¤

---

## ğŸ” ë¹„ì§€ë„í•™ìŠµì˜ ì„¸ê³„ë¡œ ì—¬í–‰í•˜ê¸°

### ğŸŒŸ ë¹„ì§€ë„í•™ìŠµì˜ ë³¸ì§ˆì  ì´í•´

ë¹„ì§€ë„í•™ìŠµì„ ì´í•´í•˜ê¸° ìœ„í•´ í•œ ê°€ì§€ ë¹„ìœ ë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ìƒˆë¡œìš´ ë„ì‹œì— ë„ì°©í–ˆë‹¤ê³  ìƒìƒí•´ë³´ì„¸ìš”. ì§€ë„ë„ ì—†ê³ , ì•ˆë‚´ì„œë„ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë©°ì¹  ë™ì•ˆ ê±¸ì–´ë‹¤ë‹ˆë©° ê´€ì°°í•˜ë‹¤ ë³´ë©´, ìì—°ìŠ¤ëŸ½ê²Œ ìƒì—…ì§€êµ¬, ì£¼ê±°ì§€êµ¬, ê³µì› ë“±ì˜ íŒ¨í„´ì„ ë°œê²¬í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ë¹„ì§€ë„í•™ìŠµì˜ í•µì‹¬ì…ë‹ˆë‹¤.

ë¹„ì§€ë„í•™ìŠµì€ **ì •ë‹µì´ ì—†ëŠ” ìƒí™©ì—ì„œ ë°ì´í„° ìì²´ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°ë¥¼ ë°œê²¬í•˜ëŠ” í•™ìŠµ ë°©ë²•**ì…ë‹ˆë‹¤. ì¸ê°„ì´ ì„¸ìƒì„ ì´í•´í•˜ëŠ” ë°©ì‹ê³¼ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. ì•„ê¸°ê°€ ì£¼ë³€ í™˜ê²½ì„ ê´€ì°°í•˜ë©° ìŠ¤ìŠ¤ë¡œ ì‚¬ë¬¼ì˜ ë²”ì£¼ë¥¼ ë§Œë“¤ì–´ê°€ëŠ” ê²ƒì²˜ëŸ¼, ë¹„ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ì—ì„œ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

```ascii
Unsupervised Learning Discovery Process
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ” Raw Data Observation                            â”‚
â”‚  â”œâ”€â”€ No labels, no guidance                         â”‚
â”‚  â”œâ”€â”€ Pure pattern recognition                       â”‚
â”‚  â””â”€â”€ Self-organized learning                        â”‚
â”‚                                                     â”‚
â”‚  ğŸ§© Pattern Discovery                               â”‚
â”‚  â”œâ”€â”€ Hidden structures emerge                       â”‚
â”‚  â”œâ”€â”€ Natural groupings form                         â”‚
â”‚  â””â”€â”€ Relationships become clear                     â”‚
â”‚                                                     â”‚
â”‚  ğŸ’¡ Insight Generation                              â”‚
â”‚  â”œâ”€â”€ New understanding of data                      â”‚
â”‚  â”œâ”€â”€ Unexpected connections                         â”‚
â”‚  â””â”€â”€ Foundation for further analysis                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§: ë°ì´í„°ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ê·¸ë£¹ ì°¾ê¸°

í´ëŸ¬ìŠ¤í„°ë§ì€ ë¹„ì§€ë„í•™ìŠµì˜ ê°€ì¥ ì§ê´€ì ì¸ ì‘ìš© ë¶„ì•¼ì…ë‹ˆë‹¤. ë§ˆì¹˜ íŒŒí‹°ì—ì„œ ì‚¬ëŒë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë¹„ìŠ·í•œ ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ê·¸ë£¹ìœ¼ë¡œ ëª¨ì´ëŠ” ê²ƒì²˜ëŸ¼, ë°ì´í„°ë„ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ê°€ì§„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

#### **K-Means í´ëŸ¬ìŠ¤í„°ë§ì˜ ì§ê´€ì  ì´í•´**

K-Means ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì„ ìƒê°í•´ë³´ì„¸ìš”. ë‹¹ì‹ ì´ ë„ì‹œ ê³„íšìë¼ê³  ê°€ì •í•˜ê³ , ì‹œë¯¼ë“¤ì˜ ê±°ì£¼ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ìœ„ì¹˜ì— í•™êµë¥¼ ê±´ì„¤í•´ì•¼ í•©ë‹ˆë‹¤. K-MeansëŠ” ì •í™•íˆ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.

```python
# K-Means í´ëŸ¬ìŠ¤í„°ë§ì˜ ë‹¨ê³„ë³„ êµ¬í˜„ê³¼ ì´í•´
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

class KMeansVisualizer:
    """
    K-Means ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ê³¼ì •ì„ ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤
    ê° ë‹¨ê³„ë³„ë¡œ ì–´ë–»ê²Œ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì´ ì´ë™í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤
    """
    
    def __init__(self, n_clusters=3, random_state=42):
        self.n_clusters = n_clusters
        self.random_state = random_state
        
    def demonstrate_kmeans_process(self, X):
        """
        K-Meansì˜ ë°˜ë³µì  ê°œì„  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
        
        1ë‹¨ê³„: ë¬´ì‘ìœ„ ì¤‘ì‹¬ì  ì„¤ì •
        2ë‹¨ê³„: ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹
        3ë‹¨ê³„: ê° ê·¸ë£¹ì˜ ì¤‘ì‹¬ ì¬ê³„ì‚°
        4ë‹¨ê³„: ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ
        """
        # ì´ˆê¸° ì¤‘ì‹¬ì ì„ ë¬´ì‘ìœ„ë¡œ ì„¤ì •
        centroids = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]
        
        max_iterations = 10
        for iteration in range(max_iterations):
            # ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹
            distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
            labels = np.argmin(distances, axis=0)
            
            # ìƒˆë¡œìš´ ì¤‘ì‹¬ì  ê³„ì‚°
            new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(self.n_clusters)])
            
            # ìˆ˜ë ´ ì¡°ê±´ í™•ì¸
            if np.allclose(centroids, new_centroids):
                print(f"ì•Œê³ ë¦¬ì¦˜ì´ {iteration + 1}ë²ˆì§¸ ë°˜ë³µì—ì„œ ìˆ˜ë ´í–ˆìŠµë‹ˆë‹¤!")
                break
                
            centroids = new_centroids
            
        return labels, centroids

# ì‹¤ì œ ë°ì´í„°ë¡œ ì‹œì—°í•´ë³´ê¸°
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
visualizer = KMeansVisualizer(n_clusters=4)
labels, centroids = visualizer.demonstrate_kmeans_process(X)
```

ì´ ê³¼ì •ì„ í†µí•´ ìš°ë¦¬ëŠ” K-Meansê°€ ì–´ë–»ê²Œ **ë°˜ë³µì  ê°œì„ **ì„ í†µí•´ ìµœì ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì•„ê°€ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì¹˜ ë¬¼ì´ ë‚®ì€ ê³³ìœ¼ë¡œ íë¥´ëŠ” ê²ƒì²˜ëŸ¼, ì•Œê³ ë¦¬ì¦˜ì€ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ì¥ ì¢‹ì€ í•´ë‹µì„ í–¥í•´ ìˆ˜ë ´í•©ë‹ˆë‹¤.

#### **ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì˜ ë‚˜ë¬´ êµ¬ì¡° ì‚¬ê³ **

ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì€ ì „í˜€ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§ˆì¹˜ ê°€ì¡± ì¡±ë³´ë‚˜ ìƒë¬¼í•™ì  ë¶„ë¥˜ì²´ê³„ì²˜ëŸ¼, ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ ë” í° ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ë‚˜ê°€ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

class HierarchicalClusteringExplorer:
    """
    ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì˜ ë´ë“œë¡œê·¸ë¨ ìƒì„±ê³¼ í•´ì„ì„ ë„ì™€ì£¼ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, linkage_method='ward'):
        self.linkage_method = linkage_method
        
    def create_dendrogram(self, X, labels=None):
        """
        ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì˜ ë´ë“œë¡œê·¸ë¨ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
        
        ë´ë“œë¡œê·¸ë¨ì„ ì½ëŠ” ë°©ë²•:
        - ì•„ë˜ì—ì„œ ìœ„ë¡œ ì˜¬ë¼ê°ˆìˆ˜ë¡ ë” í° ê·¸ë£¹
        - ê°€ë¡œì„ ì˜ ë†’ì´ëŠ” í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬
        - ì„¸ë¡œì„ ì€ í´ëŸ¬ìŠ¤í„° ë³‘í•© ê³¼ì •
        """
        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        distances = pdist(X)
        
        # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        linkage_matrix = linkage(distances, method=self.linkage_method)
        
        # ë´ë“œë¡œê·¸ë¨ ìƒì„±
        plt.figure(figsize=(12, 8))
        dendrogram(linkage_matrix, labels=labels)
        plt.title('ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë´ë“œë¡œê·¸ë¨')
        plt.xlabel('ë°ì´í„° í¬ì¸íŠ¸')
        plt.ylabel('í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬')
        
        return linkage_matrix

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
explorer = HierarchicalClusteringExplorer()
linkage_matrix = explorer.create_dendrogram(X)
```

### ğŸ”„ ì°¨ì› ì¶•ì†Œ: ê³ ì°¨ì› ë°ì´í„°ì˜ í•µì‹¬ ì¶”ì¶œ

ì°¨ì› ì¶•ì†ŒëŠ” ë¹„ì§€ë„í•™ìŠµì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ì˜ì—­ì…ë‹ˆë‹¤. ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë¹„ìœ ë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ 3D ì¡°ê°ìƒì„ 2D ì‚¬ì§„ìœ¼ë¡œ ì°ì„ ë•Œ, ëª¨ë“  ì •ë³´ë¥¼ ìœ ì§€í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ë“¤ì€ ë³´ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œê°€ ë°”ë¡œ ì´ëŸ° ê³¼ì •ì…ë‹ˆë‹¤.

#### **ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì˜ ì§ê´€ì  ì´í•´**

PCAëŠ” ë°ì´í„°ì˜ **ì£¼ìš” ë³€í™” ë°©í–¥**ì„ ì°¾ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ë§ˆì¹˜ êµ°ì¤‘ì´ ì›€ì§ì´ëŠ” ì£¼ëœ ë°©í–¥ì„ íŒŒì•…í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
from sklearn.decomposition import PCA
import numpy as np

class PCAVisualizer:
    """
    PCAì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì‹œê°í™”í•˜ê³  ì´í•´ë¥¼ ë•ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, n_components=2):
        self.n_components = n_components
        self.pca = PCA(n_components=n_components)
        
    def explain_variance_analysis(self, X):
        """
        PCAì˜ ë¶„ì‚° ì„¤ëª… ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
        
        í•µì‹¬ ê°œë…:
        - ë¶„ì‚°ì´ í° ë°©í–¥ = ë°ì´í„°ì˜ ì£¼ìš” ë³€í™” ë°©í–¥
        - ì£¼ì„±ë¶„ = ë°ì´í„° ë³€í™”ì˜ ì£¼ëœ ì¶•
        - ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ = ê° ì£¼ì„±ë¶„ì˜ ì¤‘ìš”ë„
        """
        # PCA ì ìš©
        X_transformed = self.pca.fit_transform(X)
        
        # ê° ì£¼ì„±ë¶„ì˜ ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨
        explained_variance_ratio = self.pca.explained_variance_ratio_
        
        print("=== PCA ë¶„ì„ ê²°ê³¼ ===")
        for i, ratio in enumerate(explained_variance_ratio):
            print(f"ì£¼ì„±ë¶„ {i+1}: {ratio:.3f} ({ratio*100:.1f}%ì˜ ë¶„ì‚° ì„¤ëª…)")
            
        total_variance = np.sum(explained_variance_ratio)
        print(f"\nì´ ì„¤ëª… ë¶„ì‚°: {total_variance:.3f} ({total_variance*100:.1f}%)")
        
        # ì£¼ì„±ë¶„ì˜ ë°©í–¥ ë²¡í„° (ì–´ë–¤ ì›ë³¸ íŠ¹ì„±ë“¤ì´ ì¤‘ìš”í•œì§€)
        components = self.pca.components_
        print("\n=== ì£¼ì„±ë¶„ì˜ êµ¬ì„± ===")
        for i, component in enumerate(components):
            print(f"ì£¼ì„±ë¶„ {i+1}ì˜ ë°©í–¥: {component}")
            
        return X_transformed, explained_variance_ratio

# ê³ ì°¨ì› ë°ì´í„° ìƒì„± (ì˜ˆ: 10ì°¨ì›)
np.random.seed(42)
X_high_dim = np.random.randn(100, 10)

# PCA ì ìš©í•´ë³´ê¸°
pca_viz = PCAVisualizer(n_components=3)
X_reduced, variance_ratios = pca_viz.explain_variance_analysis(X_high_dim)

print(f"\nì›ë³¸ ë°ì´í„° ì°¨ì›: {X_high_dim.shape}")
print(f"ì¶•ì†Œëœ ë°ì´í„° ì°¨ì›: {X_reduced.shape}")
```

#### **t-SNE: ë³µì¡í•œ êµ¬ì¡°ì˜ ì‹œê°í™”**

t-SNEëŠ” PCAì™€ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§ˆì¹˜ ë³µì¡í•œ ì¢…ì´ë¥¼ í‰ë©´ì— í¼ì³ë†“ë˜, ê°€ê¹Œìš´ ì ë“¤ì€ ê°€ê¹ê²Œ, ë¨¼ ì ë“¤ì€ ë©€ê²Œ ìœ ì§€í•˜ë ¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
from sklearn.manifold import TSNE

class TSNEExplorer:
    """
    t-SNEì˜ íŠ¹ì„±ê³¼ ì ì ˆí•œ ì‚¬ìš©ë²•ì„ ì´í•´í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, perplexity=30, n_iter=1000):
        self.perplexity = perplexity
        self.n_iter = n_iter
        
    def compare_perplexity_effects(self, X):
        """
        t-SNEì˜ perplexity ë§¤ê°œë³€ìˆ˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¹„êµ
        
        perplexity ì´í•´í•˜ê¸°:
        - ì‘ì€ ê°’ (5-10): ì§€ì—­ì  êµ¬ì¡° ê°•ì¡°
        - ì¤‘ê°„ ê°’ (20-50): ê· í˜•ì¡íŒ ì‹œê°í™”
        - í° ê°’ (50-100): ì „ì—­ì  êµ¬ì¡° ê°•ì¡°
        """
        perplexity_values = [5, 30, 50]
        results = {}
        
        for perp in perplexity_values:
            print(f"perplexity = {perp}ë¡œ t-SNE ì‹¤í–‰ ì¤‘...")
            
            tsne = TSNE(n_components=2, perplexity=perp, 
                       n_iter=self.n_iter, random_state=42)
            X_embedded = tsne.fit_transform(X)
            
            results[perp] = X_embedded
            
            # ìˆ˜ë ´ ì •ë³´ ì¶œë ¥
            print(f"  ìµœì¢… KL divergence: {tsne.kl_divergence_:.3f}")
            
        return results

# ì‹¤ì œ ê³ ì°¨ì› ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
tsne_explorer = TSNEExplorer()
embedding_results = tsne_explorer.compare_perplexity_effects(X_high_dim)
```

---

## ğŸ® ê°•í™”í•™ìŠµì˜ ê²Œì„ ê°™ì€ ì„¸ê³„

### ğŸŒŸ ê°•í™”í•™ìŠµì˜ ë³¸ì§ˆì  ì² í•™

ê°•í™”í•™ìŠµì„ ì´í•´í•˜ê¸° ìœ„í•´ ì•„ì´ê°€ ìì „ê±°ë¥¼ ë°°ìš°ëŠ” ê³¼ì •ì„ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ì´ëŠ” ë„˜ì–´ì§ˆ ë•Œë§ˆë‹¤ ì•„í”„ë‹¤ëŠ” ê²ƒì„ ë°°ìš°ê³ , ê· í˜•ì„ ì¡ì„ ë•Œë§ˆë‹¤ ê¸°ì¨ì„ ëŠë‚ë‹ˆë‹¤. ì´ëŸ° ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ì ì  ë” ì˜ íƒ€ê²Œ ë©ë‹ˆë‹¤. ê°•í™”í•™ìŠµì´ ë°”ë¡œ ì´ëŸ° ë°©ì‹ì…ë‹ˆë‹¤.

ê°•í™”í•™ìŠµì€ **í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ìµœì ì˜ í–‰ë™ ì „ëµì„ í•™ìŠµí•˜ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤. ì •ë‹µì´ ì£¼ì–´ì§€ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œ ë³´ìƒê³¼ ì²˜ë²Œì„ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

```ascii
Reinforcement Learning Cycle
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– Agent (Learning Entity)                        â”‚
â”‚  â”œâ”€â”€ Current State: What's happening now?          â”‚
â”‚  â”œâ”€â”€ Action: What should I do?                     â”‚
â”‚  â””â”€â”€ Policy: My strategy for action selection      â”‚
â”‚                                                     â”‚
â”‚  ğŸŒ Environment (The World)                        â”‚
â”‚  â”œâ”€â”€ State Transition: How world changes           â”‚
â”‚  â”œâ”€â”€ Reward Signal: Feedback for actions           â”‚
â”‚  â””â”€â”€ Rules: What's possible and what's not         â”‚
â”‚                                                     â”‚
â”‚  ğŸ”„ Learning Loop                                   â”‚
â”‚  â”œâ”€â”€ Observe â†’ Decide â†’ Act â†’ Receive Feedback     â”‚
â”‚  â”œâ”€â”€ Update Knowledge â†’ Improve Strategy            â”‚
â”‚  â””â”€â”€ Repeat until mastery                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Q-Learning: ê°€ì¹˜ ê¸°ë°˜ í•™ìŠµì˜ í•µì‹¬

Q-Learningì„ ì´í•´í•˜ê¸° ìœ„í•´ ë¯¸ë¡œë¥¼ íƒˆì¶œí•˜ëŠ” ìƒí™©ì„ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ê° ìœ„ì¹˜ì—ì„œ ê° ë°©í–¥ìœ¼ë¡œ ì´ë™í–ˆì„ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” **ì˜ˆìƒ ë³´ìƒ**ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ Q-Learningì˜ í•µì‹¬ì…ë‹ˆë‹¤.

```python
import numpy as np
import matplotlib.pyplot as plt

class QLearningMaze:
    """
    Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ ë¯¸ë¡œ íƒˆì¶œ ë¬¸ì œë¡œ ì´í•´í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, maze_size=5, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):
        self.maze_size = maze_size
        self.learning_rate = learning_rate  # í•™ìŠµë¥ : ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë°›ì•„ë“¤ì¼ì§€
        self.discount_factor = discount_factor  # í• ì¸ìœ¨: ë¯¸ë˜ ë³´ìƒì„ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ë³¼ì§€
        self.epsilon = epsilon  # íƒí—˜ìœ¨: ìƒˆë¡œìš´ í–‰ë™ì„ ì‹œë„í•  í™•ë¥ 
        
        # Q-í…Œì´ë¸” ì´ˆê¸°í™” (ìƒíƒœ x í–‰ë™)
        self.q_table = np.zeros((maze_size * maze_size, 4))  # 4ë°©í–¥ (ìƒí•˜ì¢Œìš°)
        
        # í–‰ë™ ì •ì˜
        self.actions = {
            0: (-1, 0),  # ìœ„ë¡œ
            1: (1, 0),   # ì•„ë˜ë¡œ
            2: (0, -1),  # ì™¼ìª½ìœ¼ë¡œ
            3: (0, 1)    # ì˜¤ë¥¸ìª½ìœ¼ë¡œ
        }
        
    def state_to_position(self, state):
        """ìƒíƒœ ë²ˆí˜¸ë¥¼ (í–‰, ì—´) ì¢Œí‘œë¡œ ë³€í™˜"""
        return (state // self.maze_size, state % self.maze_size)
        
    def position_to_state(self, row, col):
        """(í–‰, ì—´) ì¢Œí‘œë¥¼ ìƒíƒœ ë²ˆí˜¸ë¡œ ë³€í™˜"""
        return row * self.maze_size + col
        
    def get_reward(self, state):
        """ê° ìƒíƒœì—ì„œì˜ ë³´ìƒì„ ì •ì˜"""
        row, col = self.state_to_position(state)
        
        # ëª©í‘œ ì§€ì  (ì˜¤ë¥¸ìª½ ì•„ë˜ ëª¨ì„œë¦¬)
        if row == self.maze_size - 1 and col == self.maze_size - 1:
            return 100  # í° ë³´ìƒ
            
        # ì¥ì• ë¬¼ (ì˜ˆ: íŠ¹ì • ìœ„ì¹˜ë“¤)
        obstacles = [(1, 1), (2, 2), (3, 1)]
        if (row, col) in obstacles:
            return -10  # ì²˜ë²Œ
            
        # ì¼ë°˜ì ì¸ ì´ë™
        return -1  # ì‹œê°„ ë¹„ìš©
        
    def choose_action(self, state):
        """Îµ-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ"""
        if np.random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ í–‰ë™
            return np.random.randint(4)
        else:
            # í™œìš©: ê°€ì¥ ì¢‹ì€ í–‰ë™
            return np.argmax(self.q_table[state])
            
    def update_q_value(self, state, action, reward, next_state):
        """Q-ê°’ ì—…ë°ì´íŠ¸ (ë²¨ë§Œ ë°©ì •ì‹ ì ìš©)"""
        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])
        
        # ë²¨ë§Œ ë°©ì •ì‹: Q(s,a) â† Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state, action] = new_q
        
    def train(self, episodes=1000):
        """Q-Learning í›ˆë ¨ ê³¼ì •"""
        episode_rewards = []
        
        for episode in range(episodes):
            # ì‹œì‘ ìœ„ì¹˜ (ì™¼ìª½ ìœ„ ëª¨ì„œë¦¬)
            current_state = 0
            total_reward = 0
            steps = 0
            
            while steps < 50:  # ìµœëŒ€ 50ìŠ¤í…ìœ¼ë¡œ ì œí•œ
                # í–‰ë™ ì„ íƒ
                action = self.choose_action(current_state)
                
                # í–‰ë™ ì‹¤í–‰
                row, col = self.state_to_position(current_state)
                dr, dc = self.actions[action]
                new_row, new_col = row + dr, col + dc
                
                # ê²½ê³„ í™•ì¸
                if (0 <= new_row < self.maze_size and 
                    0 <= new_col < self.maze_size):
                    next_state = self.position_to_state(new_row, new_col)
                else:
                    next_state = current_state  # ë²½ì— ë¶€ë”ªíˆë©´ ì œìë¦¬
                
                # ë³´ìƒ ê³„ì‚°
                reward = self.get_reward(next_state)
                total_reward += reward
                
                # Q-ê°’ ì—…ë°ì´íŠ¸
                self.update_q_value(current_state, action, reward, next_state)
                
                current_state = next_state
                steps += 1
                
                # ëª©í‘œ ë„ë‹¬ ì‹œ ì—í”¼ì†Œë“œ ì¢…ë£Œ
                if reward == 100:
                    break
                    
            episode_rewards.append(total_reward)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if episode % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"ì—í”¼ì†Œë“œ {episode}: í‰ê·  ë³´ìƒ = {avg_reward:.2f}")
                
        return episode_rewards
        
    def visualize_policy(self):
        """í•™ìŠµëœ ì •ì±… ì‹œê°í™”"""
        policy_map = np.zeros((self.maze_size, self.maze_size))
        
        for state in range(self.maze_size * self.maze_size):
            row, col = self.state_to_position(state)
            best_action = np.argmax(self.q_table[state])
            policy_map[row, col] = best_action
            
        # í™”ì‚´í‘œë¡œ ì •ì±… í‘œì‹œ
        direction_symbols = ['â†‘', 'â†“', 'â†', 'â†’']
        
        print("\n=== í•™ìŠµëœ ì •ì±… (ìµœì  í–‰ë™) ===")
        for row in range(self.maze_size):
            for col in range(self.maze_size):
                action = int(policy_map[row, col])
                print(direction_symbols[action], end='  ')
            print()

# Q-Learning ì‹¤ìŠµ
maze_learner = QLearningMaze(maze_size=5)
rewards = maze_learner.train(episodes=1000)

# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
maze_learner.visualize_policy()

# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 6))
plt.plot(rewards)
plt.title('Q-Learning í•™ìŠµ ê³¡ì„ ')
plt.xlabel('ì—í”¼ì†Œë“œ')
plt.ylabel('ì´ ë³´ìƒ')
plt.grid(True)
plt.show()
```

### ğŸ² ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸: ì§ì ‘ì ì¸ ì „ëµ í•™ìŠµ

ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ë°©ë²•ì€ Q-Learningê³¼ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°€ì¹˜ í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , **ì§ì ‘ì ìœ¼ë¡œ í–‰ë™ ì„ íƒ ì „ëµ**ì„ í•™ìŠµí•©ë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class PolicyGradientAgent:
    """
    ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ì—ì´ì „íŠ¸
    """
    
    def __init__(self, state_size, action_size, hidden_size=64, learning_rate=0.01):
        self.state_size = state_size
        self.action_size = action_size
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ (ìƒíƒœ â†’ í–‰ë™ í™•ë¥ )
        self.policy_network = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_size),
            nn.Softmax(dim=-1)
        )
        
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)
        
        # ì—í”¼ì†Œë“œ ë™ì•ˆ ì €ì¥í•  ì •ë³´
        self.log_probs = []
        self.rewards = []
        
    def select_action(self, state):
        """ìƒíƒœì— ë”°ë¥¸ í–‰ë™ ì„ íƒ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ í–‰ë™ í™•ë¥  ê³„ì‚°
        action_probs = self.policy_network(state_tensor)
        
        # í™•ë¥  ë¶„í¬ì—ì„œ í–‰ë™ ìƒ˜í”Œë§
        distribution = Categorical(action_probs)
        action = distribution.sample()
        
        # ë‚˜ì¤‘ì— í•™ìŠµí•  ë•Œ ì‚¬ìš©í•  log probability ì €ì¥
        self.log_probs.append(distribution.log_prob(action))
        
        return action.item()
        
    def update_policy(self, gamma=0.99):
        """ì •ì±… ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        # í• ì¸ëœ ëˆ„ì  ë³´ìƒ ê³„ì‚°
        discounted_rewards = []
        cumulative_reward = 0
        
        # ë’¤ì—ì„œë¶€í„° ê³„ì‚° (ë¯¸ë˜ ë³´ìƒì„ í• ì¸)
        for reward in reversed(self.rewards):
            cumulative_reward = reward + gamma * cumulative_reward
            discounted_rewards.insert(0, cumulative_reward)
            
        # ë³´ìƒ ì •ê·œí™” (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (
            discounted_rewards.std() + 1e-8
        )
        
        # ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        policy_loss = []
        for log_prob, reward in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * reward)
            
        policy_loss = torch.stack(policy_loss).sum()
        
        # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()
        
        # ì €ì¥ëœ ì •ë³´ ì´ˆê¸°í™”
        self.log_probs = []
        self.rewards = []
        
    def add_reward(self, reward):
        """ë³´ìƒ ì¶”ê°€"""
        self.rewards.append(reward)

# ê°„ë‹¨í•œ í™˜ê²½ì—ì„œ ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
class SimpleEnvironment:
    """
    ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í™˜ê²½
    - ìƒíƒœ: [ìœ„ì¹˜, ì†ë„]
    - í–‰ë™: [ì™¼ìª½ìœ¼ë¡œ, ì˜¤ë¥¸ìª½ìœ¼ë¡œ]
    - ëª©í‘œ: íŠ¹ì • ìœ„ì¹˜ì— ë„ë‹¬í•˜ê¸°
    """
    
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.position = 0.0
        self.velocity = 0.0
        self.target = 5.0
        return np.
